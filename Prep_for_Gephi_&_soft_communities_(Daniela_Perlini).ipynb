{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries**"
      ],
      "metadata": {
        "id": "hQSuKPIYhO7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.sparse as sps\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install louvain\n",
        "import louvain\n",
        "import igraph as ig\n",
        "import time\n",
        "import pickle\n",
        "import csv"
      ],
      "metadata": {
        "id": "F-pCUreahJFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88786552-ce6f-4ec1-ca8b-a5dd52ac399a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-igraph\n",
            "  Downloading python_igraph-0.11.4-py3-none-any.whl (9.1 kB)\n",
            "Collecting igraph==0.11.4 (from python-igraph)\n",
            "  Downloading igraph-0.11.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.2 (from igraph==0.11.4->python-igraph)\n",
            "  Using cached texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph, python-igraph\n",
            "Successfully installed igraph-0.11.4 python-igraph-0.11.4 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEZ2M8s_3fls",
        "outputId": "2756b3e8-1445-4b64-897b-5b09eb230986"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Reddit data**"
      ],
      "metadata": {
        "id": "kY3nCVTYhppF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to visualize the graph of words instead of the one of documents, so\n",
        "#firstly we load the words' occurrences already cleaned and prepped\n",
        "in_dir = \"drive/MyDrive/Colab Notebooks/IP-NS/drive_comments/\"\n",
        "in_file = \"reddit_titles_posts_final_nohindi_parent\"\n",
        "\n",
        "Mwd, words, documents = pickle.load(open(in_dir+in_file+\"_occurrences.p\",\"rb\"))"
      ],
      "metadata": {
        "id": "6gxY-AnVhpLz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of words that appear without the subsequent label \"ADV\" or \"VERB\" etc\n",
        "WORDS=[w.split()[0] for w in words.copy()]"
      ],
      "metadata": {
        "id": "zO8riuhq88L8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=Mwd.shape[0] #number of words like \"word LABEL\" considered by the occurrency matrix Mwd\n",
        "m=Mwd.shape[1] #number of documents considered by Mwd\n",
        "WORDSUNIQUE=[] #introducing the list of unique words in WORDS\n",
        "Owd=np.zeros((1,m)) #introducing a new occurrency matrix that counts just words in WORDSUNIQUE\n",
        "cont=0"
      ],
      "metadata": {
        "id": "xj25LDhP8E6u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructing both Owd and WORDSUNIQUE as defined above:\n",
        "imax=-1\n",
        "\n",
        "for i in range(imax+1,n):\n",
        "  if i==cont:\n",
        "    word=WORDS[i]\n",
        "    WORDSUNIQUE.append(word)\n",
        "    for j in range(i,n):\n",
        "      if word!=WORDS[j]:\n",
        "        cont=j\n",
        "        break\n",
        "      Owd[-1,:]+=Mwd[j,:]\n",
        "    Owd=np.vstack((Owd,np.zeros((1,m))))\n",
        "  #if i==8000:  # this snippet of code was added in case Colab was unable to run it all in one go,\n",
        "                #allowing it to break the process in multiple loops by manually updating imax and\n",
        "                #rerunning the code each time\n",
        "  #  break"
      ],
      "metadata": {
        "id": "Ib7AaWpNRoIx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying that the last line of Owd is made of zeros:\n",
        "print((Owd[-1]==np.zeros(m)).sum()==m)\n",
        "\n",
        "# Deleting the last line of Owd:\n",
        "Owd=Owd[:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GlALuOHIpvb",
        "outputId": "51355e88-2350-4114-b253-846041504c31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build probability matrices from words occurrences**"
      ],
      "metadata": {
        "id": "fTqsFYsip3D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"drive/MyDrive/Colab Notebooks/IP-NS/communities_mod.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HToGt0n0hk3u",
        "outputId": "c9baabd0-cd67-49a8-a2b4-acd7f53f4be1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bertopic 1.11\n",
            "softlouvain v1.10\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning occurrency matrix Owd by removing nodes with too high or too low of a degree:\n",
        "Owd = sps.csr_matrix(Owd)\n",
        "WORDSUNIQUE = np.array(WORDSUNIQUE)\n",
        "Owd, WORDSUNIQUE, documents = clean_Mwd_matrix(Owd, WORDSUNIQUE, documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zAG0jZyGDBh",
        "outputId": "c4fcf23b-ac07-461a-e14f-27ae282ab1c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removing: change climate  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building probability matrices starting from Owd:\n",
        "POwd, POww, POdd, POaa = probability_matrices(Owd, tform=False)"
      ],
      "metadata": {
        "id": "Qc2jXzVsKGNh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to build on Gephi a graph representing the relationships between words,\n",
        "#so we set the nodes to be the words and the edges between them to have weights equal\n",
        "#to the corresponding entries of the POww matrix. Thus, the degree of a node is the\n",
        "#sum of the entries of the corresponding row of POww.\n",
        "\n",
        "# Node IDs and labels:\n",
        "node_ids = list(range(POww.shape[0]))\n",
        "node_labels = list(WORDSUNIQUE)\n",
        "node_degrees = [sum((POww[i,:]).toarray())[0] for i in range(POww.shape[0])]\n",
        "\n",
        "# Combining node IDs, labels and degrees data into a list of tuples:\n",
        "rows = zip(node_ids, node_labels, node_degrees)"
      ],
      "metadata": {
        "id": "E84kcAlocePq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the CSV file path:\n",
        "csv_file_path = in_dir+in_file+'_outputwords1.csv'\n",
        "\n",
        "R = list(rows)\n",
        "W=R"
      ],
      "metadata": {
        "id": "x31roIZl9ahX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing CSV file with all the nodes' data:\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Header:\n",
        "    csv_writer.writerow(['Id', 'Label','Degree'])\n",
        "\n",
        "    # Writing data:\n",
        "    csv_writer.writerows((W))\n",
        "\n",
        "print(\"CSV file written successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACPg8GhgrsYb",
        "outputId": "abd96cbd-0eb5-4a3d-a44f-b8cda83ea354"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file written successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for the edges, we introduce source and target to be lists storing the nodes' ids\n",
        "#corresponding to each certain edge, while also saving this edge's weight into the weights list:\n",
        "source=[]\n",
        "target=[]\n",
        "weight=[]\n",
        "for i in range(POww.shape[0]):\n",
        "  for j in range(i,POww.shape[0]):\n",
        "    if POww[i,j]!=0:\n",
        "      source.append(i)\n",
        "      target.append(j)\n",
        "      weight.append(POww[i,j])"
      ],
      "metadata": {
        "id": "kTUr7UKEJmxj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the data we've extracted into a list of tuples:\n",
        "rows = zip(source, target, weight)\n",
        "R=list(rows)\n",
        "W=np.array(W)\n",
        "nodes_selected=W[:,0] #this is just in case we had previously filtered some more the nodes to be considered\n",
        "Y=[]\n",
        "for i in range(len(R)):\n",
        "  if (str(R[i][0]) in nodes_selected) and (str(R[i][1]) in nodes_selected):\n",
        "    Y.append(R[i])"
      ],
      "metadata": {
        "id": "ZvHAaDzWsxrB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing CSV file with all the edges' data:\n",
        "csv_file_path = in_dir+in_file+'_outputwordsedges.csv'\n",
        "\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Header:\n",
        "    csv_writer.writerow(['Source', 'Target', 'Weight'])\n",
        "\n",
        "    # Writing data:\n",
        "    csv_writer.writerows(Y)"
      ],
      "metadata": {
        "id": "JqWRnjCYd_U8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have all the files to correctly visualize the graph on Gephi"
      ],
      "metadata": {
        "id": "lnf8RWAnCzIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assign documents to topics using Louvain and soft Louvain**\n",
        "i.e., run Louvain community detection on POdd"
      ],
      "metadata": {
        "id": "yIS4iAcbixPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start a time counter\n",
        "tic = time.time()\n",
        "\n",
        "# build a graph based on Pdd as adjacency matrix\n",
        "A = sps.csr_matrix(POdd)\n",
        "G = ig.Graph.Adjacency((A > 0).toarray().tolist())\n",
        "G.es['weight'] = np.array(A[A.nonzero()])[0]\n",
        "\n",
        "# run Louvain on the graph to get a partition\n",
        "part = louvain.find_partition(G, louvain.ModularityVertexPartition,\n",
        "                                 weights='weight')\n",
        "\n",
        "# function to map the partition into a community assignment matrix C\n",
        "# where rows represent documents, and columns represent topics\n",
        "\n",
        "def partition_to_C(part):\n",
        "  C = sps.csr_matrix((POdd.shape[0],len(part)))\n",
        "  for i in range(len(part)):\n",
        "    C[np.array(part[i]),i] = 1\n",
        "  return C\n",
        "\n",
        "# map the partition into a community assignment matrix C\n",
        "C_l = partition_to_C(part)\n",
        "\n",
        "# capture execution time\n",
        "et_louv = time.time()-tic\n"
      ],
      "metadata": {
        "id": "8LeQbFhe7qgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c32c18-84f5-48cb-d713-29005c7ab605"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.time()\n",
        "# refine with soft Louvain\n",
        "C_sl, _, _ = my_soft_louvain(POdd, C_l)\n",
        "\n",
        "# capture execution time\n",
        "et_slouv = time.time()-tic"
      ],
      "metadata": {
        "id": "c8D_x66jcrk6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking here that my_soft_louvain actually finds overlapping communities when applied to the (hard)\n",
        "#community assignment matrix found by the louvain algorithm:\n",
        "\n",
        "comparisons=[[i for i,j in zip(*C_sl.nonzero()) if j==n] for n in range(C_sl.shape[1])]\n",
        "setcomparisons=[set(o) for o in comparisons] #this list stores the sets of indices of elements in C_sl\n",
        "#that are non-zero. Each set correspond to each community found by my_soft_louvain.\n",
        "\n",
        "comp=[setcomparisons[i].intersection(setcomparisons[j]) for i in range(C_sl.shape[1]) for j in range(i+1,C_sl.shape[1])]\n",
        "#comp is a list storing the intersections between the sets in setcomparisons, taken two by two.\n",
        "#Thus if a document was assigned to at least 2 different communities, it will definitely appear in comp.\n",
        "\n",
        "# Checking which kind of non-empty interesections, if any, are there:\n",
        "print([o for o in comp if o!=set()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRgd3Aw-DFt1",
        "outputId": "4d316fcf-9d6f-4f49-b0fc-edf81973faca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since apparently even C_sl was a hard community assignment matrix, here we try applying\n",
        "#my_soft_louvain to the trivial community assignment matrix (each document represent 1 community):\n",
        "C_soft, _, prov = my_soft_louvain(POdd)"
      ],
      "metadata": {
        "id": "jW1a42yeOen0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the intersections of the sets of non zero indices regarding C_soft:\n",
        "comparisons=[[i for i,j in zip(*C_soft.nonzero()) if j==n] for n in range(C_soft.shape[1])]\n",
        "setcomparisons=[set(o) for o in comparisons]\n",
        "comp=[setcomparisons[i].intersection(setcomparisons[j]) for i in range(C_soft.shape[1]) for j in range(i+1,C_soft.shape[1])]\n",
        "\n",
        "# Checking which kind of non-empty intersections, if any, are there:\n",
        "print([o for o in comp if o!=set()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVLUU7wxPMwz",
        "outputId": "d44d35c0-0487-48b7-de8b-a2a376cc6029"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing that for the dataset we've acquired, my_soft_louvain doesn't find overlapping communities,\n",
        "#but rather just makes slight adjustments to the C_l found by louvain, the need to actually account for\n",
        "#overlapping communities arises. Therefore we've decided to implement the algorithm of BigClam that should\n",
        "#do exactly that. In the end we'll compare the original louvain to BigClam, without considering my_soft_louvain."
      ],
      "metadata": {
        "id": "q3EoDD0MI6eD"
      },
      "execution_count": 27,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}